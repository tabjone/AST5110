{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretization of a function of a single variable. Differentiation through finite differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 – Code to represent the function and the analytical and numerical derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$h(x) = \\cos\\left[\\frac{\\pi (x-1)}{2}\\right] \\exp\\left[-\\left(\\frac{x-3}{2.5}\\right)^2\\right],\\tag{1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $x \\in (-4,10) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a sampling of $h$ with 64 intervals, that is, 65 points, and store the\n",
    "values into double precision arrays called $xx$ and $hh$. Numpy arrays are double precision as \n",
    "default. To define $xx$ in Python you can use the commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import numpy as np\n",
    "nump=65\n",
    "x0=-4.0 \n",
    "xf=10.0\n",
    "xx = np.arange(nump)/(nump-1.0) * (xf-x0) + x0`\n",
    "\n",
    "Use `matplotlib.pyplot` to visualize hh vs xx. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the variable $nint$ as the number of intervals ($nint= 64$ in the present case)\n",
    "and $nump$ as the number of points. In IDL, Python and C, those components go\n",
    "from the $0-$component through the component $nump−1$. Compute the ratio (1) in the [wiki](https://github.com/AST-Course/AST5110/wiki/Discretization) using and filling the function `deriv_dnw` in `nm_lib`. Feel free to use any known library or create your own functions from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will depend on how you created the function that you have $nump$ or $nump−1$ elements. If the former, the last component ($nump-1$) is ill calculated. $hp$ contains a second-order approximation to the derivative of the $hh$ function at the intermediate points $x_{i+1/2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nm_lib import nm_lib as nm\n",
    "#Creating name db for float64 to save space. 64bit = double point precision.\n",
    "db = np.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">JMS</span>.\n",
    "\n",
    "<span style=\"color:yellow\">very minor comment, if the machine is 64 bits it will probably be float64 as deafult</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Plot $hh$ versus $xx$ as a solid line with crosses added at each grid point (to visualize the goodness of the discretization) or with `plt.hist` function combined with `plt.plot`. _Make sure the axis pixels are properly located either to the center or half grid shifted_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x):\n",
    "    \"\"\"This is function h(x) from eq. (1)\"\"\"\n",
    "    return np.cos(np.pi * (x-db(1))/db(2)) * np.exp(-((x-db(3))/db(2.5))**2)\n",
    "\n",
    "def dh_dx(x):\n",
    "    \"\"\"Derivative of h(x) w.r.t. x.\"\"\"\n",
    "    a = np.pi/db(2)\n",
    "    b = db(2.5)\n",
    "    return (np.exp(-(x - db(3))**2/b**2) * (a * b**2* np.sin(a - a *x) - db(2) *(x - db(3)) *np.cos(a *(x - db(1)))))/b**2\n",
    "\n",
    "def get_xx_hh_dx(nump, x0, xf):\n",
    "    \"\"\"This function creates the array of gridpoints for x. Then solves h(x) and creates a\n",
    "       delta x. And this is xx[1]-xx[0] because it is the same for all of the grid.\"\"\"\n",
    "    xx = np.arange(nump)/(nump-db(1.0)) * (xf-x0) + x0\n",
    "    hh = h(xx)\n",
    "    dx = np.abs(xx[1] - xx[0])\n",
    "    return xx, hh, dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Plot the array containing the numerical derivative, $hp$. Calculate analytically the derivative of the function (1) and represent it in the same figure to ascertain the goodness of the approximation for that number of points. __hint__ _make sure the axis pixels are properly located either to the center or half grid shifted_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_derivative(xx, hh, dx):\n",
    "    \"\"\"Return numerical and analytical\n",
    "       ha analytic, hp numerical\n",
    "    \"\"\"\n",
    "    dx = np.roll(xx,-1) - xx\n",
    "    \n",
    "    hp = nm.deriv_dnw(xx, hh, **{\"ddx_order\":1})\n",
    "    ha = dh_dx(xx + dx/2)\n",
    "    return hp, ha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using get_xx_hh_dx function to get variables for different grid sizes, 16, 32, and 64 size grids.\n",
    "xx_16, hh_16, dx_16 = get_xx_hh_dx(nump=16, x0=db(-4.0), xf=db(10.0))\n",
    "xx_32, hh_32, dx_32 = get_xx_hh_dx(nump=32, x0=db(-4.0), xf=db(10.0))\n",
    "xx_64, hh_64, dx_64 = get_xx_hh_dx(nump=64, x0=db(-4.0), xf=db(10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binplot(xx, hh, dx):\n",
    "    \"\"\"This function creates the histogram vs the function on the left\n",
    "       and analytical derivative vs numerical on the right.\"\"\"\n",
    "    counts, bins = np.histogram(hh)\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(8,4))\n",
    "    ax[0].set_title(\"Function\")\n",
    "    ax[0].set_xlabel(\"x\")\n",
    "    ax[0].set_ylabel(\"h(x)\")\n",
    "    ax[0].step(xx + dx/db(2), hh, label=\"Bins\")\n",
    "    ax[0].plot(xx, hh, \"x-\", label=\"Func\")\n",
    "    ax[0].legend()\n",
    "    \n",
    "    hp, ha = calculate_derivative(xx, hh, dx)\n",
    "    ax[1].set_title(\"Derivative\")\n",
    "    ax[1].set_xlabel(\"x\")\n",
    "    ax[1].set_ylabel(\"dh/dx\")\n",
    "    ax[1].plot(xx, hp, \"x\", label=\"Numerical\")\n",
    "    ax[1].plot(xx, ha, label=\"Analytical\")\n",
    "    ax[1].legend()\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binplot(xx_64, hh_64, dx_64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">JMS</span>.\n",
    "\n",
    "<span style=\"color:blue\">Good</span>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first plot we see h(x) as described in equation (1). This is discretized so we have plotted the bins to show this. In the right plot I have taken the first order downwind derivative on h(x) and also calculated the analytical dhdx and given this the gridspace as an argument. We had to shift the bins by a half step to make the middle of the gridpoint match the function. This also had to be done for the analytical derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Repeat the foregoing, but now using $nint= 32$ and $nint= 16$ intervals to see how the approximation deteriorates. Thereafter, repeat the same process for 128 and 256 intervals, to see how it improves. Consider to use `plt.semilogy` for the error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binplot(xx_32, hh_32, dx_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binplot(xx_16, hh_16, dx_16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">JMS</span>.\n",
    "\n",
    "<span style=\"color:blue\">Good</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps done are the same as in the first plot but now we have lowered the resolution. In the first plot we had 64 gridpoints and now we have gone down to 32 and finally 16. We can see that the derivative get's a little more off the more we lower the resolution. Expecially at the extreme points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Test of the quadratic order of the approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to test if the ratio $(h_{i+1}-h_i)/(x_{i+1}-x_i)$ approaches the analytical value of the derivative. To that end, we will use samplings with, successively, 16, 32, 64, 128, 256, 512 and 1024 intervals (which are successive powers of 2). Calculate the maximum of the absolute value of the error, meaning: the difference between the analytical and the numerical derivatives at the _same points_. Plot a graph of that value versus the size of the interval in each case using a diagram with logarithmic axes. Check if the curve you get corresponds to a quadratic dependence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a list of nint to go trough and append the error values for all of these. The error is the sum of the\n",
    "#absolute values between the analytical and numerical derivative.\n",
    "ntests = [16, 32, 64, 128, 256, 512, 1024]\n",
    "errors = []\n",
    "\n",
    "for ntest in ntests:\n",
    "    xx, hh, dx = get_xx_hh_dx(nump=ntest, x0=db(-4.0), xf=db(10.0))\n",
    "    hp, ha = calculate_derivative(xx, hh, dx)\n",
    "    plt.semilogy(xx, np.abs(hp-ha), label=str(ntest))\n",
    "    errors.append(np.mean(np.abs(hp-ha)))\n",
    "plt.xlabel(\"xx\")\n",
    "plt.ylabel(\"abs(hp-ha)\")\n",
    "plt.legend()\n",
    "plt.ylim((1e-8,2e-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">JMS</span>.\n",
    "\n",
    "<span style=\"color:blue\">Good</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have plotted the absolute value between the analytical solution and the numerical solution. We can see that as the gridpoints increase the error get's about $1/10$ smaller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(ntests, errors)\n",
    "ax.set_xlabel(\"nump\")\n",
    "ax.set_ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Improving the accuracy of the test of the quadratic order of the approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the accuracy of the result of the previous paragraph:\n",
    "\n",
    "1. extend the test to a larger range of number of intervals (including 2048, 4096, 8192, 16384). Make sure to use double precision variables throughout the program (meaning: all variables except the array indices).\n",
    "\n",
    "2. then try to fit a straight to the logarithm of the error curves using Python program `numpy.polyfit` and `numpy.poly1d`. From the value of the slope you get from that program, check the accuracy with which you obtain the quadratic dependence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we add some more values to the inveral and calculate the new errors\n",
    "new_errors = errors\n",
    "intervals = [2048, 4096, 8192, 16384]\n",
    "\n",
    "new_interval = ntests + intervals\n",
    "\n",
    "for ntest in intervals:\n",
    "    xx, hh, dx = get_xx_hh_dx(nump=ntest, x0=db(-4.0), xf=db(10.0))\n",
    "    hp, ha = calculate_derivative(xx, hh, dx)\n",
    "    new_errors.append(np.mean(np.abs(hp-ha)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We go into log-space\n",
    "log_error = np.log10(new_errors)\n",
    "log_interval = np.log10(new_interval)\n",
    "#doing the polynomial fitting to degree 1.\n",
    "coefficients = np.polyfit(log_interval, log_error, 1)\n",
    "#Need only coefficients but create fit variable for plotting\n",
    "fit = np.poly1d(coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.loglog(ntests+intervals, new_errors, label=\"Error\")\n",
    "ax.loglog(10**(log_interval), 10**(fit(log_interval)), label=\"Fit\")\n",
    "ax.set_xlabel(\"nump\")\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $∆x$ get's small enough we get floating point errors. That's why it's not linear anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When something is linear in logspace it implies that it follows a power law. \n",
    "$$\n",
    "y = c x^k.\n",
    "$$\n",
    "\n",
    "$$\n",
    "log(y)=log(cx^k) = log(c)+k log(x)\n",
    "$$\n",
    "\n",
    "We have found the coefficients for log(y). So we have found log(c) and k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">JMS</span>.\n",
    "\n",
    "<span style=\"color:blue\">Good</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have taken the mean of the absolute value between the analytical solution and the numerical solution for increased gridpoints xx called nump in the plot. Then I have taken this into logspace and done a polynomial fit to the first degree. Then plotting the coefficients we get that the first coefficient is -2, which means that this curve goes as $\\Delta x ^2$. The reason the coefficient is negative is because the function is decreasing with nump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Analytical proof of the order of convergence of the approximation for the derivative (optional)\n",
    "\n",
    "Consider the sampling used in exercise this, assuming that the spacing between grid points is uniform, i.e., $(\\Delta x)_i = \\Delta x$. Write a formal Taylor expansion as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x_{i+1}) = f(x_{i+1/2}) + f'(x_{i+1/2})\\frac{\\Delta x}{2} + ...  \\tag{2}$$\n",
    "\n",
    "$$f(x_{i}) = f(x_{i+1/2}) - f'(x_{i+1/2})\\frac{\\Delta x}{2} + ...  \\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "including terms up to order $(\\Delta x)^3$. Eliminating terms combining those two expressions, conclude that, as said in the previous exercise sheet, the finite-difference approximation to the derivative at the midpoints $x_{i+1/2}$ carried out there is of 2nd order."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1feae36462ef2989ec41c4a51b5942270fcc0f857ea15d353eae93dcd2e6cf1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
