{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nm_lib import nm_lib as nm\n",
    "import threading\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from matplotlib import animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Space-time partial differential equation: Study of the diffusive equation (implicit methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider now the viscous term in Burger's equation: \n",
    "\n",
    "$$\\frac{\\partial u}{\\partial t} = \\nu \\frac{\\partial^2 u}{\\partial x^2}, \\tag{1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Apply an explicit method. \n",
    "\n",
    "What would be the CFL condition for a viscous term where $\\nu$ is either a constant or an array that depends on $x$. We would like to solve equation (1) numerically for $x  [x_0, x_f]$ with $x_0 = âˆ’2.6$, $x_f = 2.6$, periodic boundary conditions and with the initial condition:\n",
    "\n",
    "$$u(x,t=t_0) = A\\exp(-(x-x_0)^2/W^2)   \\tag{2}$$\n",
    "\n",
    "with $A=0.3$, $W=0.1$, and $x_0=0$. __Suggestion__: Apply the first derivative upwind and the second downwind. Apply Von Newman analysis. Is it stable? What is the time-step dependence with $\\Delta x$? \n",
    "\n",
    "How many steps are needed to reach a $t=1.8$ for $nump=128$? And $256$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def u(x, A, W):\n",
    "    x0 = 0\n",
    "    return A * np.exp(-(-x-x0)**2/W**2)\n",
    "\n",
    "x0 = -2.6\n",
    "xf = 2.6\n",
    "\n",
    "def get_xx(x0, xf, nump):\n",
    "    return np.arange(nump)/(nump-1.0) * (xf-x0) + x0\n",
    "\n",
    "A = 0.3\n",
    "W = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The viscous term using Forward-Euler in time and First upwind then downwind in space:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{u_j^{n+1}-u_j^n}{\\Delta t} &= \\nu \\frac{(u_{j+1}^n - 2u_j^n + u_{j-1}^n)}{\\Delta x^2}.\\\\\n",
    "\\end{align}\n",
    "\n",
    "Now Von Neumann analysis of the viscous term, first assume peculiar solution on the form\n",
    "\n",
    "$$\n",
    "u_j^n = \\xi^n e^{ikj\\Delta x}.\n",
    "$$\n",
    "\n",
    "Inserting this in the equation above gives\n",
    "\\begin{align}\n",
    "e^{ikj\\Delta x}\\frac{\\xi^{n+1}-\\xi^{n}}{\\Delta t} &= \\nu \\xi^n e^{ikj\\Delta x} \\frac{e^{ik\\Delta x}-2+e^{-ik\\Delta x}}{\\Delta x^2}\\\\\n",
    "\\frac{\\xi^{n+1}-\\xi^{n}}{\\xi^n} &=   \\frac{\\nu\\Delta t}{\\Delta x^2}\\left(e^{ik\\Delta x}-2+e^{-ik\\Delta x}\\right)\\\\\n",
    "\\xi &=\\frac{\\nu\\Delta t}{\\Delta x^2}\\left(\\cos{(k\\Delta x)}+i\\sin{(k\\Delta x)} -2+\\cos{(k\\Delta x)-i\\sin{(k\\Delta x)}} \\right) +1\\\\\n",
    "\\xi &=\\frac{2\\nu\\Delta t}{\\Delta x^2}\\left(\\cos{(k\\Delta x)} -1 \\right) +1\\\\\n",
    "\\xi &=-\\frac{4\\nu\\Delta t}{\\Delta x^2}\\sin^2{\\left(\\frac{k\\Delta x}{2}\\right)} + 1\n",
    "\\end{align}\n",
    "\n",
    "We require that $|\\xi^n|^2\\leq 1$ for all $k$. This gives\n",
    "\n",
    "\\begin{align}\n",
    "\\left|-\\frac{4\\nu\\Delta t}{\\Delta x^2}\\sin^2{\\left(\\frac{k\\Delta x}{2}\\right)} + 1 \\right|^2 &\\leq 1\\\\\n",
    "\\frac{16\\nu^2\\Delta t^2}{\\Delta x^4}\\sin^4{\\left(\\frac{k\\Delta x}{2}\\right)} - \\frac{4\\nu\\Delta t}{\\Delta x^2}\\sin^2{\\left(\\frac{k\\Delta x}{2}\\right)} + 1 &\\leq 1\n",
    "\\end{align}\n",
    "\n",
    "This needs to hold for all $k$, so we look at the max of the sine function\n",
    "\n",
    "$$\n",
    "\\frac{16\\nu^2\\Delta t^2}{\\Delta x^4} - \\frac{4\\nu\\Delta t}{\\Delta x^2} + 1 \\leq 1\n",
    "$$\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{16\\nu^2\\Delta t^2}{\\Delta x^4} &\\leq \\frac{4\\nu\\Delta t}{\\Delta x^2}\\\\\n",
    "\\frac{4\\nu \\Delta t}{\\Delta x^2} &\\leq 1\n",
    "\\end{align}\n",
    "\n",
    "$$\n",
    "\\Delta t \\leq \\frac{\\Delta x^2}{4\\nu}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = get_xx(x0, xf, nump=256)\n",
    "hh = u(xx, A, W)\n",
    "nt = 30\n",
    "dt = 1.1\n",
    "a = 1\n",
    "\n",
    "tt_diff, unnt_diff = nm.evolv_diff_burgers(xx, hh,nt, a, cfl_cut = 0.98)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))\n",
    "def init(): \n",
    "    axes.plot(xx,unnt_diff[0,:])\n",
    "    axes.set_ylim(-0.05, 0.32)\n",
    "    axes.grid(True)\n",
    "\n",
    "def animate(i):\n",
    "    axes.clear()\n",
    "    axes.plot(xx,unnt_diff[i,:])\n",
    "    axes.set_ylim(-0.05, 0.32)\n",
    "    axes.set_title('t={:.2f}, i={:g}'.format(tt_diff[i],i))\n",
    "    axes.grid(True)\n",
    "\n",
    "anim = FuncAnimation(fig, animate, interval=50, frames=nt, init_func=init)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good now. A little diffusive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">JMS</span>.\n",
    "\n",
    "<span style=\"color:blue\">fixed! </span>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 1e-2\n",
    "t, unnt, errt, countt = nm.Newton_Raphson(xx, hh, a, dt, nt, toll= 1e-5, ncount=30, bnd_type='wrap', bnd_limits=[1,1])\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))\n",
    "def init(): \n",
    "    axes.plot(xx,unnt[:,0])\n",
    "    axes.set_ylim(-0.05, 0.32)\n",
    "    axes.grid(True)\n",
    "\n",
    "def animate(i):\n",
    "    axes.clear()\n",
    "    axes.plot(xx,unnt[:,i])\n",
    "    axes.set_ylim(-0.05, 0.32)\n",
    "    axes.set_title('t={:.2f}, i={:g}'.format(t[i],i))\n",
    "    axes.grid(True)\n",
    "\n",
    "anim = FuncAnimation(fig, animate, interval=50, frames=nt, init_func=init)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see the Newton Rapson method. This looks much more diffusive but when we look at the time compared to diff-burgers we can see that for the diff-burgers the timesteps are so small that we are at 0.00s still with 2 significant digits because the dt become so small. With the newton rapson we can increment with much larger timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">JMS</span>.\n",
    "\n",
    "<span style=\"color:blue\"> Ok </span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose one of the following options: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Implicit methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [wiki](https://github.com/AST-Course/AST5110/wiki/Implicit-methods), we describe some implicit or semi-explicit methods that allow relaxing the CFL constraint on diffusive terms. Consider Newton-Rapson method and repeat the previous numerical experiment. For this, you will need to implement the following   \n",
    "\n",
    "\n",
    "$F_j = u^{n+1}_j - u^n_j - \\nu \\, (u^{n+1}_{j+1} - 2u^{n+1}_{j}+u^{n+1}_{j-1})\\frac{\\Delta t}{\\Delta x^2}$\n",
    "\n",
    "in `NR_f` and `step_diff_burgers` functions in `nm_lib`. \n",
    "\n",
    "And the Jacobian can be easily built. \n",
    "\n",
    "$J(j,k) = F_j'(u^{n+1}_k)$\n",
    "\n",
    "fill in the `jacobian` function in `nm_lib`. Note that this matrix is linear with $u$. \n",
    "\n",
    "Test the model with [wiki](https://github.com/AST-Course/AST5110/wiki/Self-similar-solution-for-parabolic-eq) self-similar solutions. How long it takes each time step compared to the Lax-method? Use `time.time` library. Do it for nump=256, nt=30 and dt = 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = get_xx(x0, xf, nump=256)\n",
    "hh = u(xx, A, W)\n",
    "nt = 30\n",
    "dt = 0.01\n",
    "a = 1\n",
    "\n",
    "n_times = 10\n",
    "\n",
    "diff_times = np.zeros(n_times)\n",
    "for i in range(n_times):\n",
    "    start = time.time()\n",
    "    nm.evolv_diff_burgers(xx, hh,nt, a, cfl_cut = 0.98)\n",
    "    end = time.time()\n",
    "    diff_times[i] = end-start\n",
    "\n",
    "nr_times = np.zeros(n_times)\n",
    "for i in range(n_times):\n",
    "    start = time.time()\n",
    "    nm.Newton_Raphson(xx, hh, a, dt, nt, toll= 1e-5, ncount=2, bnd_type='wrap', bnd_limits=[1,1])\n",
    "    end = time.time()\n",
    "    nr_times[i] = end - start\n",
    "    \n",
    "mean_diff = np.mean(diff_times)\n",
    "std_diff = np.std(diff_times)\n",
    "\n",
    "mean_nr = np.mean(nr_times)\n",
    "std_nr = np.std(nr_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Time diff: {:.1e}+-{:.1e}\".format(mean_diff, std_diff))\n",
    "print(\"Time NR: {:.1e}+-{:.1e}\".format(mean_nr, std_nr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolv_diff_burgers is two orders of magnitude slower than NR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">JMS</span>.\n",
    "\n",
    "<span style=\"color:blue\"> fixed.  </span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the simulation, use `curve_fit` from `scipy.optimize`. \n",
    "\n",
    "__hint__ consider to use a good initial guess (`p0`) in and `bnd_limits` to facilitate the fitting wiht `curve_fit`. What happens to the solution when increasing dt? How much can be improved in limiting the tolerance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The self similar solution says that the height of the gaussian should go like $t^{-1/2}$ and the width should go like $t^{1/2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, mu, sigma, norm):\n",
    "    p = np.exp(-np.power(x - mu, 2.) / (2 * np.power(sigma, 2.)))\n",
    "    p = p*norm\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, unnt, errt, countt = nm.Newton_Raphson(xx, hh, a, dt, nt, toll= 1e-5, ncount=15, bnd_type='wrap', bnd_limits=[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from scipy.optimize import OptimizeWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks = np.zeros(len(t))\n",
    "fwhms = np.zeros(len(t))\n",
    "\n",
    "for i in range(len(t)):\n",
    "    popt, pcov = curve_fit(gaussian, xx, unnt[:,i], p0=[0, 0.3, 0.3])\n",
    "    g = gaussian(xx, *popt)\n",
    "    fwhms[i] = popt[1]\n",
    "    peaks[i] = np.max(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_slope_and_fit(x_array, y_array):\n",
    "    # Exclude the first value in both x_array and y_array\n",
    "    # because x[0]=0 => logspace = -inf\n",
    "    x_array_excluded = x_array[1:]\n",
    "    y_array_excluded = y_array[1:]\n",
    "\n",
    "    # Convert the x and y arrays into log space\n",
    "    log_x = np.log10(x_array_excluded)\n",
    "    log_y = np.log10(y_array_excluded)\n",
    "\n",
    "    # Fit a polynomial of degree 1 (linear fit) to the log-log data\n",
    "    coeffs = np.polyfit(log_x, log_y, 1)\n",
    "\n",
    "    # The first coefficient in the output is the slope of the linear fit\n",
    "    slope = coeffs[0]\n",
    "\n",
    "    # Create the fitted y values in log space using the coefficients\n",
    "    log_y_fit = np.polyval(coeffs, log_x)\n",
    "\n",
    "    # Convert the x_fit and y_fit arrays back to the original space\n",
    "    x_fit = 10 ** log_x\n",
    "    y_fit = 10 ** log_y_fit\n",
    "\n",
    "    return slope, x_fit, y_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_fwhm, x_fit_fwhm, y_fit_fwhm = log_slope_and_fit(t, fwhms)\n",
    "slope_peaks, x_fit_peaks, y_fit_peaks = log_slope_and_fit(t, peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].loglog(t, fwhms, label=\"FWHMs\")\n",
    "ax[0].loglog(x_fit_fwhm, y_fit_fwhm, linestyle=\"--\", label=\"Polynomial fit\")\n",
    "ax[1].loglog(t, peaks, label=\"Peaks\")\n",
    "ax[1].loglog(x_fit_peaks, y_fit_peaks, linestyle=\"--\", label=\"Polynomial fit\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Slope FWHMs:{slope_fwhm}\")\n",
    "print(f\"Slope Peaks:{slope_peaks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the FWHM goes as $t^{1/2}$ and the peaks as $t^{-1/2}$ when doing a Gaussian fit on Newton-Rapson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">JMS</span>.\n",
    "\n",
    "<span style=\"color:blue\"> Ok  </span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))\n",
    "def init(): \n",
    "    axes.set_ylim(-0.05, 0.32)\n",
    "    axes.plot(xx,unnt[:,0], label=\"NR\")\n",
    "    popt, pcov = curve_fit(gaussian, xx, unnt[:,0], p0=[0, 0.3, 0.3])\n",
    "    axes.plot(xx, gaussian(xx, *popt), label=\"Gaussian fit\", linestyle=\"--\")\n",
    "    axes.grid(True)\n",
    "    axes.legend()\n",
    "\n",
    "def animate(i):\n",
    "    axes.clear()\n",
    "    axes.plot(xx,unnt[:,i], label=\"NR\")\n",
    "    axes.set_ylim(-0.05, 0.32)\n",
    "    popt, pcov = curve_fit(gaussian, xx, unnt[:,i], p0=[0, 0.3, 0.3])\n",
    "    axes.plot(xx, gaussian(xx, *popt), label=\"Gaussian fit\", linestyle=\"--\")\n",
    "    axes.grid(True)\n",
    "    axes.legend()\n",
    "    \n",
    "anim = FuncAnimation(fig, animate, interval=50, frames=nt, init_func=init)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the Gaussian follows the Newton-Rapson solution as the self-similar solution says."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">JMS</span>.\n",
    "\n",
    "<span style=\"color:blue\"> Good  </span>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a non-linear function where $\\nu$ depends on $u$. To keep it simple, solve the following: \n",
    "\n",
    "$\\frac{\\partial u}{\\partial t} = u \\frac{\\partial^2 u}{\\partial x^2}$\n",
    "\n",
    "where $\\nu_0$ is a constant and the same initial conditions as the previous exercise (fill in `Newton_Raphson_u`, `jacobian_u` and `NR_f_u`. Consider an error limit of $10^{-4}$ and compare the previous exercise (with the same error limit). How many iterations needs now the method to converge to the right solution? Why? Increase `ncount` to 1000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.1\n",
    "t, unnt, errt, countt_NR_u = nm.Newton_Raphson_u(xx, hh, dt, nt, toll= 1e-4\\\n",
    "                                                 , ncount=1000, bnd_type='wrap', bnd_limits=[1,1])\n",
    "t, unnt, errt, countt_NR = nm.Newton_Raphson(xx, hh, a, dt, nt, toll= 1e-4\\\n",
    "                                               , ncount=1000, bnd_type='wrap', bnd_limits=[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(countt_NR_u)\n",
    "print(countt_NR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, unnt, errt, countt_NR_u = nm.Newton_Raphson_u(xx, hh, dt, nt, toll= 1e-6\\\n",
    "                                                 , ncount=1000, bnd_type='wrap', bnd_limits=[1,1])\n",
    "t, unnt, errt, countt_NR = nm.Newton_Raphson(xx, hh, a, dt, nt, toll= 1e-6\\\n",
    "                                               , ncount=1000, bnd_type='wrap', bnd_limits=[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(countt_NR_u)\n",
    "print(countt_NR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the NR_u find a solution in much fewer iterations. And that the NR has much more iterations, sometimes even maxing out. Lowering the tolerance does not seem to affect NR_u, but makes NR max out the iterations every time meaning that it does not converge to a solution before it stops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">JMS</span>.\n",
    "\n",
    "<span style=\"color:blue\"> Good job!  </span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Semi-explicit methods. \n",
    "\n",
    "__a)__ Super-time-stepping (STS) schemes work for parabolic terms. STS is an API method that performs a subset of \"unstable\" intermediate steps, but the sum of all the steps is stable. Visualize how `taui_sts` varies with $nu$ and $niter$. Compare the solution with the analytical one for the final and intermediate STS steps. For the full STS steps, how improves the solution with $nu$? and $niter$? Is there a relation between the error and these two parameters, $nu$, and $niter$? For which $niter$ and $nu$ the method provides larger steps than an ordinary explicit. For this exercise, fill in `evol_sts`, and `taui_sts`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1feae36462ef2989ec41c4a51b5942270fcc0f857ea15d353eae93dcd2e6cf1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
